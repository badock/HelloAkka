

[
  
  
    
    
      {
        "title": "Welcome to Jekyll!",
        "excerpt": "You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.\n\n",
        "content": "You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.\n\nTo add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.ext and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.\n\nJekyll also offers powerful support for code snippets:\n\ndef print_hi(name)\n  puts \"Hi, #{name}\"\nend\nprint_hi('Tom')\n#=&gt; prints 'Hi, Tom' to STDOUT.\n\nCheck out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk.\n\n",
        "url": "/jekyll/update/2018/11/13/welcome-to-jekyll.html"
      },
    
  
  
  
  {
    "title": "Home",
    "excerpt": "SeDuCe is a research project that aims at building a scientific testbed to enable the study of both thermal and power management aspects in datacenters.\n",
    "content": "\n\nIntroduction\n\nObjectives\n\n\n  Developing a simple distributed applications that involves communications between its pairs\n  Getting started with the Actor model\n  Get a first experience with the Akka framework in Java\n\n\nOutline\n\n\n  Session 0: Installation of the development environment and getting started\n  Session 1: Developing a simple master/slave application\n  Session 2: Towards a Ring topology\n  Session 3: Implementing a Chord like topology\n  Session 4: Running an application on Grid’5000\n\n\nContact\n\n\n  Jonathan Pastor\n\n",
    "url": "/"
  },
  
  {
    "title": "Session 0 : Configuring the development environment and getting started with Akka",
    "excerpt": "\n",
    "content": "I- Configuring the developpment environment\n\nA- Installing dependencies\n\nbrew cask install java\nbrew install maven\n\n\nB- Installing an IDE (IntelliJ)\n\nDownload IntelliJ at the following link:\n\nhttps://www.jetbrains.com/idea/\n\nII- TOdo\n",
    "url": "/session0.html"
  },
  
  {
    "title": "Architecture",
    "excerpt": "\n",
    "content": "We have builded the ecotype cluster,\nwhich contains 48 servers, and is integrated in the Grid’5000\ninfrastructure: any Grid’5000 user can reserve servers of the\necotype cluster and conduct experiments on them by using the\nusual Grid’5000 tools. The testbed is designed for research related to\npower and thermal management in datacenters: during an experiment, a\nuser can access in real time to information regarding the temperature\nof the servers involved in its experiment, and get the power\nconsumption of any parts of the testbed (servers, switches, cooling\nsystems, etc.), or control some parameters of the testbed, such as\nsetting temperature targets for the cooling systems of the cluster.\n\nServers of the ecotype cluster are based on DELL PowerEdge\nR630 and contains a pair of Intel Xeon E5-2630L v4 CPUs (10 cores, 20\nthreads per CPU), 128GB of RAM, and 400GB Solid State Disk (SSD). The\nCPUs have been designed to have a lower power consumption than other\nCPUs of the XEON 26XX serie, with a Thermal Design Power (TDP) of\n55W. Each server is connected via two 10GBe links to the Grid’5000\nproduction network, and via a single 1GBe link to the Grid’5000\nmanagement network. For instance, the Grid’5000 production network is\nused for transferring the disk images required to deploy an experiment\nor to support communications between experimental components, while\nthe management network is mainly used by the Grid’5000 backend to\ncommunicate with management cards of servers to turn them on and off.\nAdditionally, each server is certified to work in hot environments\nwhere temperature can be up to 35°C. These hardware\nspecifications will enable users to perform experiments at different\nlevels of temperature.\n\nThe cluster is composed of 5 air-tights racks (Z1, Z2, Z3, Z4, Z5)\nbased on the Schneider Electric IN-ROW model. These\nair-tights racks are equipped with Plexiglas doors, and create a\nseparation between the air inside the racks and the air from outside\nthe racks.\n\n\nFigure1: Layout of the ecotype cluster (front view)\n\nAs shown on Figure 1, one rack (Z3) is used for the cooling the\ncluster by hosting a dedicated Central Cooling System (CCS), while\nremaining racks are computing racks and are dedicated to hosting\nservers. The racks are connected and form two alleys: a cold alley at\nthe front of servers and a hot alley at their back.\n\nAs depicted by Figure 1, each computing rack hosts 12 servers, and is\norganized following two layouts of server positions: one layout where\nservers are organised in a concentrated way with no vertical space\nbetween servers (Z1 and Z2), and a second layout where servers are\nspaced at 1U intervals (Z4 and Z5).\n\nWe have deliberately chosen to use these two layouts: they will\nenable users to study the impact of the server density over the\ntemperature and the power consumption of servers.\n\nIn addition to the servers, the cluster also contains three network\nswitches that are in charge of connecting servers to the production\nnetwork and the management network of the Grid’5000\ninfrastructure. Three racks (Z2, Z4, Z5) are hosting each one a\nnetwork switch.\n\nThe 5 racks of the cluster are based on Schneider Electric\n  IN-ROW racks. This rack model creates an inside airtight\nenvironment for servers, and guarantees that the environment outside\nthe cluster has a limited impact on temperatures inside the racks. The\ntemperature inside the cluster is regulated by the CCS, which is\nconnected to a dedicated management network and implements a service\nthat enables to remote control the cooling and to access its operating\ndata with the Simple Network Management Protocol (SNMP) protocol. The\nCCS has several temperature sensors located at different parts of the\nracks, which are in charge of checking that the temperature inside\nracks is under a specified temperature threshold. It is possible to\nchange the temperature that the CCS have to maintain inside the racks\n(Cooling Temperature Target parameter), and also change the\ntemperature of the air injected by the CCS in the cold aisle\n(Air supply Temperature Target parameter). This will, in\naddition to the fact that servers have been designed to work in hot\nenvironments, enable users perform their experiments at several levels\nof temperature.\n\nRegarding the temperature outside the cluster, it is regulated by the\nSCS which is mounted from the ceiling of the server room: the SCS is\nin charge of maintaining a constant temperature in the server room,\nand thus it prevents any event outside the racks to disturb the\nexperiments that are conducted on the SeDuCe testbed.\n\nFinally, we have installed several “Airflow management panels”\nbetween each pair of servers: they improve the cooling efficiency by\npreventing the mixing of cold air and hot air inside the racks.\n\nEnergy monitoring\n\nThe power consumption of each element composing the cluster (servers,\nnetwork switches, cooling fans, condensators, etc.) is monitored and\nstored in a database, at a frequency of one hertz (one record per\nsecond).\n\nElectrical plugs of servers and network switches are connected to\nPower Distribution Units (PDUs), which are in charge of ensuring that\nservers and network switches can meet their power needs. Each\ncomputing racks contains two PDUs, and each server of a computing rack\nhas two electrical plugs. As depicted in Figure 3, the two electrical\nplugs of a server are connected to two different PDUs, which enables\nservers to implement electrical redundancy. In turn, the PDUs share\npower consumption of servers and network switches via a dedicated\nservice network: the power consumption of each power plug can be\nfetched by issuing an SNMP request to the PDU to which it is\nconnected. In turn, the PDU provides the power consumption of each of\nits outlets.\n\nThe energy consumption of the CCS can similarly be fetched via SNMP\nrequests: the CCS implements a SNMP service which is able to provide\nthe overall power consumption and the power consumption of each of its\ninternal part such as the condensator or the fans. On the other hand,\nthe SCS does not implement any built-in networking access, and thus\ncannot share its metrics with any component over a network. To solve\nthis problem, we instrumented several parts of the SCS by using a\nFluksometer: a Fluksometer is a\nconnected device that can monitor several electrical metrics (power\nconsumption, voltage, amperage, etc.) and expose their values over a\nnetwork via a web-service at a frequency of one hertz.\n\nFinally, we have added an additional system that tracks overall power\nconsumption of servers, switches and the CCS. This additional system\nis based on the Socomec G50 metering board,\nand enables to check the soundness of the aforementioned source of\npower consumption. These additional metrics are fetched by using the\nmodbus protocol.\n\nThermal monitoring\n\nTo track the thermal behavior of the ecotype cluster, each\nserver is monitored by a pair of temperature sensors: one sensor is\npositioned at the front of the server (in the cold aisle) and another\nsensor is positioned at the back of the server (in the hot aisle).\n\n\nFigure2: Back view of a server rack\n\nAs depicted by Figure 2, each temperature\nsensor is part of a bus (based on the 1wire protocol)\nconnected to a Scanner (based on an Arduino that implements\nwifi communication) in charge of gathering data produced by\ntemperature sensors of the bus. As the front and the back of each\nserver is monitored by temperature sensors, each computing rack has in\ntotal two Scanners and two buses: a front bus for monitoring\nthe cold aisle and a back bus dedicated to the hot\naisle. Scanners fetch temperatures from their sensors at a\nfrequency of one reading per sensor every second.\n\nTemperature sensors are based on the DS18B20 sensor produced by\n“Maxim Integrated” that costs approximately 3$ per\nsensor. According to the specifications provided by the constructor,\nthe DS18B20 is able to provide a temperature reading every 750ms with\na precision of 0.5 °C between -10°C and 85°C.\n\nThe choice of the DS18B20 has been motivated by the fact that the\nDS18B20 sensor is able to work as part of an 1wire bus. In\nthe context of the SeDuCe infrastructure, 12 DS18B20 sensors are\nconnected together to form an 1wire bus, and a\nScanner, based on an nodeMCU arduino with built-in\nwifi capabilities, fetches periodically their temperature\nreadings. The current version of the firmware used by\nScanners scans an 1wire bus every second, and then pushes\ntemperature data to a Temperature Registerer service, as\nillustrated in Figure 3.\n\nWe also developed a contextualisation tool to generate firmwares for\nthe Scanners. It leverages the PlatformIO framework to program a\nScanner that pushes data to a web-service. Using this\ncontextualisation tool is simple: a developer needs to define a\nprogram template in a language close to C language and marks some\nparts of code with special tags to indicate that these parts need to\nbe contextualized with additional information, such as initializing a\nvariable with the ID of a Scanner device or with the address\nof a remote web-service (such as the one that will receive temperature\nrecords). The contextualisation tool takes this program and a context\nas input parameters, analyses the template program, and completes\nparts that requires contextualisation with information provided in the\ncontext, which results in valid C language source file. Then, the\nfirmware is compiled and automatically uploaded to Scanners\nvia their serial ports. By leveraging this contextualisation tool, we\ncan remotely configure Scanners and update their firmware\n``on the fly’’.\n\nDashboard and API\n\n\nFigure3: Architecture of the SeDuCe portal\n\nTo help users to easily access power and thermal metrics generated by\nthe SeDuCe testbed, we developed a platform that exposes publicly two\ncomponents: a web portal and an\nAPI associated to a\ndocumentation.\n\nAs illustrated by Figure 3, the web portal and\nthe API fetch data from a Time Series Database (TSDB) based on\nInfluxDB. InfluxDB enables to\nstore a large quantity of immutable time series data in a scalable\nway. In the background, InfluxDB creates aggregates of data\nby grouping periodically data from a same series. These aggregated\nsets of data enable the web portal to promptly load data used for\nvisualization.\n\n\n  Two kind of components are in charge of inserting data in the database\n  the Power consumption crawlers and the Temperature\nRegisterer. Power consumption crawlers are programs that\nare in charge of polling data from PDUs, Socomecs, Flukso, the CCS and\nthe SCS. In turn, this data is inserted in the database. On the other\nhand, the Temperature Registerer is a web service that\nreceives temperature data pushed from nodeMCU arduino\ndevices, and inserts it in the database.\n\n\nThe web portal and the API are both written in Python and leverage the\nFlask micro web framework. The API component makes an\nextensive use of the Swagger framework which\nautomatises the generation of complete REST web services and their\ndocumentations from a single description file (written in JSON or\nYAML). This choice has enabled us to focus on the definition and the\nimplementation of the API, by reducing the quantity of required\nboilerplate code.\n\nAll the components depicted in Figure 3 are implemented as\nmicro-services. Our system is able to register 200 metrics per seconds\nwith minimal hardware requirements (it is currently hosted on a single\ncomputer). In the case we add more sensors to our testbed, it is\nlikely that the existing components would be sufficient. In the case\nthat one of the component would not be able to cope with the\nadditional workload, it would be easy to setup an high availability\napproach by using a load balancer such as NGINX that would forward\nrequests to a pool of instances of the component.\n\nToward the integration of renewable energies in the testbed\n\nComing Soon\n",
    "url": "/session1.html"
  },
  
  {
    "title": "Architecture",
    "excerpt": "\n",
    "content": "We have builded the ecotype cluster,\nwhich contains 48 servers, and is integrated in the Grid’5000\ninfrastructure: any Grid’5000 user can reserve servers of the\necotype cluster and conduct experiments on them by using the\nusual Grid’5000 tools. The testbed is designed for research related to\npower and thermal management in datacenters: during an experiment, a\nuser can access in real time to information regarding the temperature\nof the servers involved in its experiment, and get the power\nconsumption of any parts of the testbed (servers, switches, cooling\nsystems, etc.), or control some parameters of the testbed, such as\nsetting temperature targets for the cooling systems of the cluster.\n\nServers of the ecotype cluster are based on DELL PowerEdge\nR630 and contains a pair of Intel Xeon E5-2630L v4 CPUs (10 cores, 20\nthreads per CPU), 128GB of RAM, and 400GB Solid State Disk (SSD). The\nCPUs have been designed to have a lower power consumption than other\nCPUs of the XEON 26XX serie, with a Thermal Design Power (TDP) of\n55W. Each server is connected via two 10GBe links to the Grid’5000\nproduction network, and via a single 1GBe link to the Grid’5000\nmanagement network. For instance, the Grid’5000 production network is\nused for transferring the disk images required to deploy an experiment\nor to support communications between experimental components, while\nthe management network is mainly used by the Grid’5000 backend to\ncommunicate with management cards of servers to turn them on and off.\nAdditionally, each server is certified to work in hot environments\nwhere temperature can be up to 35°C. These hardware\nspecifications will enable users to perform experiments at different\nlevels of temperature.\n\nThe cluster is composed of 5 air-tights racks (Z1, Z2, Z3, Z4, Z5)\nbased on the Schneider Electric IN-ROW model. These\nair-tights racks are equipped with Plexiglas doors, and create a\nseparation between the air inside the racks and the air from outside\nthe racks.\n\n\nFigure1: Layout of the ecotype cluster (front view)\n\nAs shown on Figure 1, one rack (Z3) is used for the cooling the\ncluster by hosting a dedicated Central Cooling System (CCS), while\nremaining racks are computing racks and are dedicated to hosting\nservers. The racks are connected and form two alleys: a cold alley at\nthe front of servers and a hot alley at their back.\n\nAs depicted by Figure 1, each computing rack hosts 12 servers, and is\norganized following two layouts of server positions: one layout where\nservers are organised in a concentrated way with no vertical space\nbetween servers (Z1 and Z2), and a second layout where servers are\nspaced at 1U intervals (Z4 and Z5).\n\nWe have deliberately chosen to use these two layouts: they will\nenable users to study the impact of the server density over the\ntemperature and the power consumption of servers.\n\nIn addition to the servers, the cluster also contains three network\nswitches that are in charge of connecting servers to the production\nnetwork and the management network of the Grid’5000\ninfrastructure. Three racks (Z2, Z4, Z5) are hosting each one a\nnetwork switch.\n\nThe 5 racks of the cluster are based on Schneider Electric\n  IN-ROW racks. This rack model creates an inside airtight\nenvironment for servers, and guarantees that the environment outside\nthe cluster has a limited impact on temperatures inside the racks. The\ntemperature inside the cluster is regulated by the CCS, which is\nconnected to a dedicated management network and implements a service\nthat enables to remote control the cooling and to access its operating\ndata with the Simple Network Management Protocol (SNMP) protocol. The\nCCS has several temperature sensors located at different parts of the\nracks, which are in charge of checking that the temperature inside\nracks is under a specified temperature threshold. It is possible to\nchange the temperature that the CCS have to maintain inside the racks\n(Cooling Temperature Target parameter), and also change the\ntemperature of the air injected by the CCS in the cold aisle\n(Air supply Temperature Target parameter). This will, in\naddition to the fact that servers have been designed to work in hot\nenvironments, enable users perform their experiments at several levels\nof temperature.\n\nRegarding the temperature outside the cluster, it is regulated by the\nSCS which is mounted from the ceiling of the server room: the SCS is\nin charge of maintaining a constant temperature in the server room,\nand thus it prevents any event outside the racks to disturb the\nexperiments that are conducted on the SeDuCe testbed.\n\nFinally, we have installed several “Airflow management panels”\nbetween each pair of servers: they improve the cooling efficiency by\npreventing the mixing of cold air and hot air inside the racks.\n\nEnergy monitoring\n\nThe power consumption of each element composing the cluster (servers,\nnetwork switches, cooling fans, condensators, etc.) is monitored and\nstored in a database, at a frequency of one hertz (one record per\nsecond).\n\nElectrical plugs of servers and network switches are connected to\nPower Distribution Units (PDUs), which are in charge of ensuring that\nservers and network switches can meet their power needs. Each\ncomputing racks contains two PDUs, and each server of a computing rack\nhas two electrical plugs. As depicted in Figure 3, the two electrical\nplugs of a server are connected to two different PDUs, which enables\nservers to implement electrical redundancy. In turn, the PDUs share\npower consumption of servers and network switches via a dedicated\nservice network: the power consumption of each power plug can be\nfetched by issuing an SNMP request to the PDU to which it is\nconnected. In turn, the PDU provides the power consumption of each of\nits outlets.\n\nThe energy consumption of the CCS can similarly be fetched via SNMP\nrequests: the CCS implements a SNMP service which is able to provide\nthe overall power consumption and the power consumption of each of its\ninternal part such as the condensator or the fans. On the other hand,\nthe SCS does not implement any built-in networking access, and thus\ncannot share its metrics with any component over a network. To solve\nthis problem, we instrumented several parts of the SCS by using a\nFluksometer: a Fluksometer is a\nconnected device that can monitor several electrical metrics (power\nconsumption, voltage, amperage, etc.) and expose their values over a\nnetwork via a web-service at a frequency of one hertz.\n\nFinally, we have added an additional system that tracks overall power\nconsumption of servers, switches and the CCS. This additional system\nis based on the Socomec G50 metering board,\nand enables to check the soundness of the aforementioned source of\npower consumption. These additional metrics are fetched by using the\nmodbus protocol.\n\nThermal monitoring\n\nTo track the thermal behavior of the ecotype cluster, each\nserver is monitored by a pair of temperature sensors: one sensor is\npositioned at the front of the server (in the cold aisle) and another\nsensor is positioned at the back of the server (in the hot aisle).\n\n\nFigure2: Back view of a server rack\n\nAs depicted by Figure 2, each temperature\nsensor is part of a bus (based on the 1wire protocol)\nconnected to a Scanner (based on an Arduino that implements\nwifi communication) in charge of gathering data produced by\ntemperature sensors of the bus. As the front and the back of each\nserver is monitored by temperature sensors, each computing rack has in\ntotal two Scanners and two buses: a front bus for monitoring\nthe cold aisle and a back bus dedicated to the hot\naisle. Scanners fetch temperatures from their sensors at a\nfrequency of one reading per sensor every second.\n\nTemperature sensors are based on the DS18B20 sensor produced by\n“Maxim Integrated” that costs approximately 3$ per\nsensor. According to the specifications provided by the constructor,\nthe DS18B20 is able to provide a temperature reading every 750ms with\na precision of 0.5 °C between -10°C and 85°C.\n\nThe choice of the DS18B20 has been motivated by the fact that the\nDS18B20 sensor is able to work as part of an 1wire bus. In\nthe context of the SeDuCe infrastructure, 12 DS18B20 sensors are\nconnected together to form an 1wire bus, and a\nScanner, based on an nodeMCU arduino with built-in\nwifi capabilities, fetches periodically their temperature\nreadings. The current version of the firmware used by\nScanners scans an 1wire bus every second, and then pushes\ntemperature data to a Temperature Registerer service, as\nillustrated in Figure 3.\n\nWe also developed a contextualisation tool to generate firmwares for\nthe Scanners. It leverages the PlatformIO framework to program a\nScanner that pushes data to a web-service. Using this\ncontextualisation tool is simple: a developer needs to define a\nprogram template in a language close to C language and marks some\nparts of code with special tags to indicate that these parts need to\nbe contextualized with additional information, such as initializing a\nvariable with the ID of a Scanner device or with the address\nof a remote web-service (such as the one that will receive temperature\nrecords). The contextualisation tool takes this program and a context\nas input parameters, analyses the template program, and completes\nparts that requires contextualisation with information provided in the\ncontext, which results in valid C language source file. Then, the\nfirmware is compiled and automatically uploaded to Scanners\nvia their serial ports. By leveraging this contextualisation tool, we\ncan remotely configure Scanners and update their firmware\n``on the fly’’.\n\nDashboard and API\n\n\nFigure3: Architecture of the SeDuCe portal\n\nTo help users to easily access power and thermal metrics generated by\nthe SeDuCe testbed, we developed a platform that exposes publicly two\ncomponents: a web portal and an\nAPI associated to a\ndocumentation.\n\nAs illustrated by Figure 3, the web portal and\nthe API fetch data from a Time Series Database (TSDB) based on\nInfluxDB. InfluxDB enables to\nstore a large quantity of immutable time series data in a scalable\nway. In the background, InfluxDB creates aggregates of data\nby grouping periodically data from a same series. These aggregated\nsets of data enable the web portal to promptly load data used for\nvisualization.\n\n\n  Two kind of components are in charge of inserting data in the database\n  the Power consumption crawlers and the Temperature\nRegisterer. Power consumption crawlers are programs that\nare in charge of polling data from PDUs, Socomecs, Flukso, the CCS and\nthe SCS. In turn, this data is inserted in the database. On the other\nhand, the Temperature Registerer is a web service that\nreceives temperature data pushed from nodeMCU arduino\ndevices, and inserts it in the database.\n\n\nThe web portal and the API are both written in Python and leverage the\nFlask micro web framework. The API component makes an\nextensive use of the Swagger framework which\nautomatises the generation of complete REST web services and their\ndocumentations from a single description file (written in JSON or\nYAML). This choice has enabled us to focus on the definition and the\nimplementation of the API, by reducing the quantity of required\nboilerplate code.\n\nAll the components depicted in Figure 3 are implemented as\nmicro-services. Our system is able to register 200 metrics per seconds\nwith minimal hardware requirements (it is currently hosted on a single\ncomputer). In the case we add more sensors to our testbed, it is\nlikely that the existing components would be sufficient. In the case\nthat one of the component would not be able to cope with the\nadditional workload, it would be easy to setup an high availability\napproach by using a load balancer such as NGINX that would forward\nrequests to a pool of instances of the component.\n\nToward the integration of renewable energies in the testbed\n\nComing Soon\n",
    "url": "/session2.html"
  },
  
  {
    "title": "Architecture",
    "excerpt": "\n",
    "content": "We have builded the ecotype cluster,\nwhich contains 48 servers, and is integrated in the Grid’5000\ninfrastructure: any Grid’5000 user can reserve servers of the\necotype cluster and conduct experiments on them by using the\nusual Grid’5000 tools. The testbed is designed for research related to\npower and thermal management in datacenters: during an experiment, a\nuser can access in real time to information regarding the temperature\nof the servers involved in its experiment, and get the power\nconsumption of any parts of the testbed (servers, switches, cooling\nsystems, etc.), or control some parameters of the testbed, such as\nsetting temperature targets for the cooling systems of the cluster.\n\nServers of the ecotype cluster are based on DELL PowerEdge\nR630 and contains a pair of Intel Xeon E5-2630L v4 CPUs (10 cores, 20\nthreads per CPU), 128GB of RAM, and 400GB Solid State Disk (SSD). The\nCPUs have been designed to have a lower power consumption than other\nCPUs of the XEON 26XX serie, with a Thermal Design Power (TDP) of\n55W. Each server is connected via two 10GBe links to the Grid’5000\nproduction network, and via a single 1GBe link to the Grid’5000\nmanagement network. For instance, the Grid’5000 production network is\nused for transferring the disk images required to deploy an experiment\nor to support communications between experimental components, while\nthe management network is mainly used by the Grid’5000 backend to\ncommunicate with management cards of servers to turn them on and off.\nAdditionally, each server is certified to work in hot environments\nwhere temperature can be up to 35°C. These hardware\nspecifications will enable users to perform experiments at different\nlevels of temperature.\n\nThe cluster is composed of 5 air-tights racks (Z1, Z2, Z3, Z4, Z5)\nbased on the Schneider Electric IN-ROW model. These\nair-tights racks are equipped with Plexiglas doors, and create a\nseparation between the air inside the racks and the air from outside\nthe racks.\n\n\nFigure1: Layout of the ecotype cluster (front view)\n\nAs shown on Figure 1, one rack (Z3) is used for the cooling the\ncluster by hosting a dedicated Central Cooling System (CCS), while\nremaining racks are computing racks and are dedicated to hosting\nservers. The racks are connected and form two alleys: a cold alley at\nthe front of servers and a hot alley at their back.\n\nAs depicted by Figure 1, each computing rack hosts 12 servers, and is\norganized following two layouts of server positions: one layout where\nservers are organised in a concentrated way with no vertical space\nbetween servers (Z1 and Z2), and a second layout where servers are\nspaced at 1U intervals (Z4 and Z5).\n\nWe have deliberately chosen to use these two layouts: they will\nenable users to study the impact of the server density over the\ntemperature and the power consumption of servers.\n\nIn addition to the servers, the cluster also contains three network\nswitches that are in charge of connecting servers to the production\nnetwork and the management network of the Grid’5000\ninfrastructure. Three racks (Z2, Z4, Z5) are hosting each one a\nnetwork switch.\n\nThe 5 racks of the cluster are based on Schneider Electric\n  IN-ROW racks. This rack model creates an inside airtight\nenvironment for servers, and guarantees that the environment outside\nthe cluster has a limited impact on temperatures inside the racks. The\ntemperature inside the cluster is regulated by the CCS, which is\nconnected to a dedicated management network and implements a service\nthat enables to remote control the cooling and to access its operating\ndata with the Simple Network Management Protocol (SNMP) protocol. The\nCCS has several temperature sensors located at different parts of the\nracks, which are in charge of checking that the temperature inside\nracks is under a specified temperature threshold. It is possible to\nchange the temperature that the CCS have to maintain inside the racks\n(Cooling Temperature Target parameter), and also change the\ntemperature of the air injected by the CCS in the cold aisle\n(Air supply Temperature Target parameter). This will, in\naddition to the fact that servers have been designed to work in hot\nenvironments, enable users perform their experiments at several levels\nof temperature.\n\nRegarding the temperature outside the cluster, it is regulated by the\nSCS which is mounted from the ceiling of the server room: the SCS is\nin charge of maintaining a constant temperature in the server room,\nand thus it prevents any event outside the racks to disturb the\nexperiments that are conducted on the SeDuCe testbed.\n\nFinally, we have installed several “Airflow management panels”\nbetween each pair of servers: they improve the cooling efficiency by\npreventing the mixing of cold air and hot air inside the racks.\n\nEnergy monitoring\n\nThe power consumption of each element composing the cluster (servers,\nnetwork switches, cooling fans, condensators, etc.) is monitored and\nstored in a database, at a frequency of one hertz (one record per\nsecond).\n\nElectrical plugs of servers and network switches are connected to\nPower Distribution Units (PDUs), which are in charge of ensuring that\nservers and network switches can meet their power needs. Each\ncomputing racks contains two PDUs, and each server of a computing rack\nhas two electrical plugs. As depicted in Figure 3, the two electrical\nplugs of a server are connected to two different PDUs, which enables\nservers to implement electrical redundancy. In turn, the PDUs share\npower consumption of servers and network switches via a dedicated\nservice network: the power consumption of each power plug can be\nfetched by issuing an SNMP request to the PDU to which it is\nconnected. In turn, the PDU provides the power consumption of each of\nits outlets.\n\nThe energy consumption of the CCS can similarly be fetched via SNMP\nrequests: the CCS implements a SNMP service which is able to provide\nthe overall power consumption and the power consumption of each of its\ninternal part such as the condensator or the fans. On the other hand,\nthe SCS does not implement any built-in networking access, and thus\ncannot share its metrics with any component over a network. To solve\nthis problem, we instrumented several parts of the SCS by using a\nFluksometer: a Fluksometer is a\nconnected device that can monitor several electrical metrics (power\nconsumption, voltage, amperage, etc.) and expose their values over a\nnetwork via a web-service at a frequency of one hertz.\n\nFinally, we have added an additional system that tracks overall power\nconsumption of servers, switches and the CCS. This additional system\nis based on the Socomec G50 metering board,\nand enables to check the soundness of the aforementioned source of\npower consumption. These additional metrics are fetched by using the\nmodbus protocol.\n\nThermal monitoring\n\nTo track the thermal behavior of the ecotype cluster, each\nserver is monitored by a pair of temperature sensors: one sensor is\npositioned at the front of the server (in the cold aisle) and another\nsensor is positioned at the back of the server (in the hot aisle).\n\n\nFigure2: Back view of a server rack\n\nAs depicted by Figure 2, each temperature\nsensor is part of a bus (based on the 1wire protocol)\nconnected to a Scanner (based on an Arduino that implements\nwifi communication) in charge of gathering data produced by\ntemperature sensors of the bus. As the front and the back of each\nserver is monitored by temperature sensors, each computing rack has in\ntotal two Scanners and two buses: a front bus for monitoring\nthe cold aisle and a back bus dedicated to the hot\naisle. Scanners fetch temperatures from their sensors at a\nfrequency of one reading per sensor every second.\n\nTemperature sensors are based on the DS18B20 sensor produced by\n“Maxim Integrated” that costs approximately 3$ per\nsensor. According to the specifications provided by the constructor,\nthe DS18B20 is able to provide a temperature reading every 750ms with\na precision of 0.5 °C between -10°C and 85°C.\n\nThe choice of the DS18B20 has been motivated by the fact that the\nDS18B20 sensor is able to work as part of an 1wire bus. In\nthe context of the SeDuCe infrastructure, 12 DS18B20 sensors are\nconnected together to form an 1wire bus, and a\nScanner, based on an nodeMCU arduino with built-in\nwifi capabilities, fetches periodically their temperature\nreadings. The current version of the firmware used by\nScanners scans an 1wire bus every second, and then pushes\ntemperature data to a Temperature Registerer service, as\nillustrated in Figure 3.\n\nWe also developed a contextualisation tool to generate firmwares for\nthe Scanners. It leverages the PlatformIO framework to program a\nScanner that pushes data to a web-service. Using this\ncontextualisation tool is simple: a developer needs to define a\nprogram template in a language close to C language and marks some\nparts of code with special tags to indicate that these parts need to\nbe contextualized with additional information, such as initializing a\nvariable with the ID of a Scanner device or with the address\nof a remote web-service (such as the one that will receive temperature\nrecords). The contextualisation tool takes this program and a context\nas input parameters, analyses the template program, and completes\nparts that requires contextualisation with information provided in the\ncontext, which results in valid C language source file. Then, the\nfirmware is compiled and automatically uploaded to Scanners\nvia their serial ports. By leveraging this contextualisation tool, we\ncan remotely configure Scanners and update their firmware\n``on the fly’’.\n\nDashboard and API\n\n\nFigure3: Architecture of the SeDuCe portal\n\nTo help users to easily access power and thermal metrics generated by\nthe SeDuCe testbed, we developed a platform that exposes publicly two\ncomponents: a web portal and an\nAPI associated to a\ndocumentation.\n\nAs illustrated by Figure 3, the web portal and\nthe API fetch data from a Time Series Database (TSDB) based on\nInfluxDB. InfluxDB enables to\nstore a large quantity of immutable time series data in a scalable\nway. In the background, InfluxDB creates aggregates of data\nby grouping periodically data from a same series. These aggregated\nsets of data enable the web portal to promptly load data used for\nvisualization.\n\n\n  Two kind of components are in charge of inserting data in the database\n  the Power consumption crawlers and the Temperature\nRegisterer. Power consumption crawlers are programs that\nare in charge of polling data from PDUs, Socomecs, Flukso, the CCS and\nthe SCS. In turn, this data is inserted in the database. On the other\nhand, the Temperature Registerer is a web service that\nreceives temperature data pushed from nodeMCU arduino\ndevices, and inserts it in the database.\n\n\nThe web portal and the API are both written in Python and leverage the\nFlask micro web framework. The API component makes an\nextensive use of the Swagger framework which\nautomatises the generation of complete REST web services and their\ndocumentations from a single description file (written in JSON or\nYAML). This choice has enabled us to focus on the definition and the\nimplementation of the API, by reducing the quantity of required\nboilerplate code.\n\nAll the components depicted in Figure 3 are implemented as\nmicro-services. Our system is able to register 200 metrics per seconds\nwith minimal hardware requirements (it is currently hosted on a single\ncomputer). In the case we add more sensors to our testbed, it is\nlikely that the existing components would be sufficient. In the case\nthat one of the component would not be able to cope with the\nadditional workload, it would be easy to setup an high availability\napproach by using a load balancer such as NGINX that would forward\nrequests to a pool of instances of the component.\n\nToward the integration of renewable energies in the testbed\n\nComing Soon\n",
    "url": "/session3.html"
  },
  
  {
    "title": "Architecture",
    "excerpt": "\n",
    "content": "We have builded the ecotype cluster,\nwhich contains 48 servers, and is integrated in the Grid’5000\ninfrastructure: any Grid’5000 user can reserve servers of the\necotype cluster and conduct experiments on them by using the\nusual Grid’5000 tools. The testbed is designed for research related to\npower and thermal management in datacenters: during an experiment, a\nuser can access in real time to information regarding the temperature\nof the servers involved in its experiment, and get the power\nconsumption of any parts of the testbed (servers, switches, cooling\nsystems, etc.), or control some parameters of the testbed, such as\nsetting temperature targets for the cooling systems of the cluster.\n\nServers of the ecotype cluster are based on DELL PowerEdge\nR630 and contains a pair of Intel Xeon E5-2630L v4 CPUs (10 cores, 20\nthreads per CPU), 128GB of RAM, and 400GB Solid State Disk (SSD). The\nCPUs have been designed to have a lower power consumption than other\nCPUs of the XEON 26XX serie, with a Thermal Design Power (TDP) of\n55W. Each server is connected via two 10GBe links to the Grid’5000\nproduction network, and via a single 1GBe link to the Grid’5000\nmanagement network. For instance, the Grid’5000 production network is\nused for transferring the disk images required to deploy an experiment\nor to support communications between experimental components, while\nthe management network is mainly used by the Grid’5000 backend to\ncommunicate with management cards of servers to turn them on and off.\nAdditionally, each server is certified to work in hot environments\nwhere temperature can be up to 35°C. These hardware\nspecifications will enable users to perform experiments at different\nlevels of temperature.\n\nThe cluster is composed of 5 air-tights racks (Z1, Z2, Z3, Z4, Z5)\nbased on the Schneider Electric IN-ROW model. These\nair-tights racks are equipped with Plexiglas doors, and create a\nseparation between the air inside the racks and the air from outside\nthe racks.\n\n\nFigure1: Layout of the ecotype cluster (front view)\n\nAs shown on Figure 1, one rack (Z3) is used for the cooling the\ncluster by hosting a dedicated Central Cooling System (CCS), while\nremaining racks are computing racks and are dedicated to hosting\nservers. The racks are connected and form two alleys: a cold alley at\nthe front of servers and a hot alley at their back.\n\nAs depicted by Figure 1, each computing rack hosts 12 servers, and is\norganized following two layouts of server positions: one layout where\nservers are organised in a concentrated way with no vertical space\nbetween servers (Z1 and Z2), and a second layout where servers are\nspaced at 1U intervals (Z4 and Z5).\n\nWe have deliberately chosen to use these two layouts: they will\nenable users to study the impact of the server density over the\ntemperature and the power consumption of servers.\n\nIn addition to the servers, the cluster also contains three network\nswitches that are in charge of connecting servers to the production\nnetwork and the management network of the Grid’5000\ninfrastructure. Three racks (Z2, Z4, Z5) are hosting each one a\nnetwork switch.\n\nThe 5 racks of the cluster are based on Schneider Electric\n  IN-ROW racks. This rack model creates an inside airtight\nenvironment for servers, and guarantees that the environment outside\nthe cluster has a limited impact on temperatures inside the racks. The\ntemperature inside the cluster is regulated by the CCS, which is\nconnected to a dedicated management network and implements a service\nthat enables to remote control the cooling and to access its operating\ndata with the Simple Network Management Protocol (SNMP) protocol. The\nCCS has several temperature sensors located at different parts of the\nracks, which are in charge of checking that the temperature inside\nracks is under a specified temperature threshold. It is possible to\nchange the temperature that the CCS have to maintain inside the racks\n(Cooling Temperature Target parameter), and also change the\ntemperature of the air injected by the CCS in the cold aisle\n(Air supply Temperature Target parameter). This will, in\naddition to the fact that servers have been designed to work in hot\nenvironments, enable users perform their experiments at several levels\nof temperature.\n\nRegarding the temperature outside the cluster, it is regulated by the\nSCS which is mounted from the ceiling of the server room: the SCS is\nin charge of maintaining a constant temperature in the server room,\nand thus it prevents any event outside the racks to disturb the\nexperiments that are conducted on the SeDuCe testbed.\n\nFinally, we have installed several “Airflow management panels”\nbetween each pair of servers: they improve the cooling efficiency by\npreventing the mixing of cold air and hot air inside the racks.\n\nEnergy monitoring\n\nThe power consumption of each element composing the cluster (servers,\nnetwork switches, cooling fans, condensators, etc.) is monitored and\nstored in a database, at a frequency of one hertz (one record per\nsecond).\n\nElectrical plugs of servers and network switches are connected to\nPower Distribution Units (PDUs), which are in charge of ensuring that\nservers and network switches can meet their power needs. Each\ncomputing racks contains two PDUs, and each server of a computing rack\nhas two electrical plugs. As depicted in Figure 3, the two electrical\nplugs of a server are connected to two different PDUs, which enables\nservers to implement electrical redundancy. In turn, the PDUs share\npower consumption of servers and network switches via a dedicated\nservice network: the power consumption of each power plug can be\nfetched by issuing an SNMP request to the PDU to which it is\nconnected. In turn, the PDU provides the power consumption of each of\nits outlets.\n\nThe energy consumption of the CCS can similarly be fetched via SNMP\nrequests: the CCS implements a SNMP service which is able to provide\nthe overall power consumption and the power consumption of each of its\ninternal part such as the condensator or the fans. On the other hand,\nthe SCS does not implement any built-in networking access, and thus\ncannot share its metrics with any component over a network. To solve\nthis problem, we instrumented several parts of the SCS by using a\nFluksometer: a Fluksometer is a\nconnected device that can monitor several electrical metrics (power\nconsumption, voltage, amperage, etc.) and expose their values over a\nnetwork via a web-service at a frequency of one hertz.\n\nFinally, we have added an additional system that tracks overall power\nconsumption of servers, switches and the CCS. This additional system\nis based on the Socomec G50 metering board,\nand enables to check the soundness of the aforementioned source of\npower consumption. These additional metrics are fetched by using the\nmodbus protocol.\n\nThermal monitoring\n\nTo track the thermal behavior of the ecotype cluster, each\nserver is monitored by a pair of temperature sensors: one sensor is\npositioned at the front of the server (in the cold aisle) and another\nsensor is positioned at the back of the server (in the hot aisle).\n\n\nFigure2: Back view of a server rack\n\nAs depicted by Figure 2, each temperature\nsensor is part of a bus (based on the 1wire protocol)\nconnected to a Scanner (based on an Arduino that implements\nwifi communication) in charge of gathering data produced by\ntemperature sensors of the bus. As the front and the back of each\nserver is monitored by temperature sensors, each computing rack has in\ntotal two Scanners and two buses: a front bus for monitoring\nthe cold aisle and a back bus dedicated to the hot\naisle. Scanners fetch temperatures from their sensors at a\nfrequency of one reading per sensor every second.\n\nTemperature sensors are based on the DS18B20 sensor produced by\n“Maxim Integrated” that costs approximately 3$ per\nsensor. According to the specifications provided by the constructor,\nthe DS18B20 is able to provide a temperature reading every 750ms with\na precision of 0.5 °C between -10°C and 85°C.\n\nThe choice of the DS18B20 has been motivated by the fact that the\nDS18B20 sensor is able to work as part of an 1wire bus. In\nthe context of the SeDuCe infrastructure, 12 DS18B20 sensors are\nconnected together to form an 1wire bus, and a\nScanner, based on an nodeMCU arduino with built-in\nwifi capabilities, fetches periodically their temperature\nreadings. The current version of the firmware used by\nScanners scans an 1wire bus every second, and then pushes\ntemperature data to a Temperature Registerer service, as\nillustrated in Figure 3.\n\nWe also developed a contextualisation tool to generate firmwares for\nthe Scanners. It leverages the PlatformIO framework to program a\nScanner that pushes data to a web-service. Using this\ncontextualisation tool is simple: a developer needs to define a\nprogram template in a language close to C language and marks some\nparts of code with special tags to indicate that these parts need to\nbe contextualized with additional information, such as initializing a\nvariable with the ID of a Scanner device or with the address\nof a remote web-service (such as the one that will receive temperature\nrecords). The contextualisation tool takes this program and a context\nas input parameters, analyses the template program, and completes\nparts that requires contextualisation with information provided in the\ncontext, which results in valid C language source file. Then, the\nfirmware is compiled and automatically uploaded to Scanners\nvia their serial ports. By leveraging this contextualisation tool, we\ncan remotely configure Scanners and update their firmware\n``on the fly’’.\n\nDashboard and API\n\n\nFigure3: Architecture of the SeDuCe portal\n\nTo help users to easily access power and thermal metrics generated by\nthe SeDuCe testbed, we developed a platform that exposes publicly two\ncomponents: a web portal and an\nAPI associated to a\ndocumentation.\n\nAs illustrated by Figure 3, the web portal and\nthe API fetch data from a Time Series Database (TSDB) based on\nInfluxDB. InfluxDB enables to\nstore a large quantity of immutable time series data in a scalable\nway. In the background, InfluxDB creates aggregates of data\nby grouping periodically data from a same series. These aggregated\nsets of data enable the web portal to promptly load data used for\nvisualization.\n\n\n  Two kind of components are in charge of inserting data in the database\n  the Power consumption crawlers and the Temperature\nRegisterer. Power consumption crawlers are programs that\nare in charge of polling data from PDUs, Socomecs, Flukso, the CCS and\nthe SCS. In turn, this data is inserted in the database. On the other\nhand, the Temperature Registerer is a web service that\nreceives temperature data pushed from nodeMCU arduino\ndevices, and inserts it in the database.\n\n\nThe web portal and the API are both written in Python and leverage the\nFlask micro web framework. The API component makes an\nextensive use of the Swagger framework which\nautomatises the generation of complete REST web services and their\ndocumentations from a single description file (written in JSON or\nYAML). This choice has enabled us to focus on the definition and the\nimplementation of the API, by reducing the quantity of required\nboilerplate code.\n\nAll the components depicted in Figure 3 are implemented as\nmicro-services. Our system is able to register 200 metrics per seconds\nwith minimal hardware requirements (it is currently hosted on a single\ncomputer). In the case we add more sensors to our testbed, it is\nlikely that the existing components would be sufficient. In the case\nthat one of the component would not be able to cope with the\nadditional workload, it would be easy to setup an high availability\napproach by using a load balancer such as NGINX that would forward\nrequests to a pool of instances of the component.\n\nToward the integration of renewable energies in the testbed\n\nComing Soon\n",
    "url": "/session4.html"
  }
  
]

